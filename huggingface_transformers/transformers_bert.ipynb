{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = \"data/bert-base-chinese\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['config.json', 'pytorch_model.bin', 'vocab.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(basedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokernizer = BertTokenizer.from_pretrained(basedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained(basedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokernizer(\"我们来试试牛逼的bert模型吧\", return_tensors=\"pt\")\n",
    "# inputs = tokernizer.tokenize(\"我们来试试牛逼的bert模型吧\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2769,  812, 3341, 6407, 6407, 4281, 6873, 4638, 8815, 8716, 3563,\n",
       "         1798, 1416,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 我 们 来 试 试 牛 逼 的 bert 模 型 吧 [SEP]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokernizer.decode(inputs[\"input_ids\"].data.cpu().numpy().reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 15, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[-1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  输入两个句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs2 = tokernizer(\"我们来试试牛逼的BERT模型吧\", \"听说BERT模型吊炸天！\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2769,  812, 3341, 6407, 6407, 4281, 6873, 4638, 8815, 8716, 3563,\n",
       "         1798, 1416,  102, 1420, 6432, 8815, 8716, 3563, 1798, 1396, 4156, 1921,\n",
       "         8013,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 我 们 来 试 试 牛 逼 的 bert 模 型 吧 [SEP] 听 说 bert 模 型 吊 炸 天 ！ [SEP]'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokernizer.decode(inputs2[\"input_ids\"].data.cpu().numpy().reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_outputs, pooled_outputs = model(**inputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 26, 768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4671, -0.2775, -0.8004,  ..., -0.3845, -0.7908, -0.0454],\n",
       "         [ 0.2140, -0.4219, -0.1619,  ..., -0.5265, -0.9865,  0.0982],\n",
       "         [ 0.4882, -1.4815, -0.8973,  ...,  0.8416,  0.2024,  0.2813],\n",
       "         ...,\n",
       "         [ 0.6722, -0.2272, -0.4810,  ..., -0.1517,  0.6246, -0.2592],\n",
       "         [-0.7090, -0.2184,  0.4602,  ...,  0.0310,  0.0080, -0.7972],\n",
       "         [-0.0265, -0.4677, -0.8057,  ..., -0.2529, -0.6123, -0.5260]]],\n",
       "       grad_fn=<AddcmulBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.2929e-02, -3.0075e-01, -5.7258e-01,  5.3962e-01,  5.7433e-01,\n",
       "         -5.8809e-01,  3.2765e-01, -1.3403e-01, -4.5887e-01,  3.9097e-01,\n",
       "         -1.1773e-01, -2.9848e-01, -1.5606e-02,  3.5034e-01, -5.9695e-02,\n",
       "         -1.5067e-01, -4.8329e-02,  1.5288e-01,  2.7167e-02,  2.4180e-01,\n",
       "         -2.2730e-01,  3.1727e-01, -5.5646e-02,  7.9556e-02, -2.0635e-01,\n",
       "         -3.5557e-01, -1.6777e-01, -3.2059e-01,  4.0694e-01, -6.0094e-01,\n",
       "         -4.8687e-01, -4.6362e-01, -1.1115e-01,  2.7846e-01,  2.8935e-01,\n",
       "         -4.6631e-01, -4.4169e-01, -9.3158e-02, -4.9807e-01, -3.9810e-01,\n",
       "         -2.6256e-01, -1.6920e-01, -4.5223e-01,  7.4979e-02, -4.0159e-02,\n",
       "          1.2917e-01,  3.0358e-01,  4.3336e-01,  8.9806e-02,  3.5380e-01,\n",
       "          1.2196e-01,  8.4199e+00, -6.2707e-02, -4.2542e-01, -6.4139e-01,\n",
       "          6.8191e-01,  1.0297e+00, -2.2472e-01, -7.0235e-02, -4.9721e-01,\n",
       "         -1.0397e-01,  2.4219e-01, -9.3619e-02, -3.3611e-01, -7.0822e-03,\n",
       "         -1.5673e-01, -7.2997e-02,  4.9070e-01, -1.6891e-02, -2.4647e-01,\n",
       "          4.9805e-01, -4.3915e-02, -7.4642e-02, -7.8899e-02, -6.6856e-02,\n",
       "          2.3462e-01, -3.5957e-01, -2.4932e-01, -2.5894e-01, -4.3582e-01,\n",
       "         -8.8873e-01,  2.6940e-01, -2.2147e-01, -4.8671e-02, -6.0830e-01,\n",
       "          3.0487e-01, -7.7223e-01, -4.1000e-01,  5.9773e-01, -6.0926e-01,\n",
       "         -2.2535e-01,  1.6663e-02, -8.5015e-02,  1.4732e-01,  5.3386e-01,\n",
       "          6.6866e-02,  2.3374e-01,  4.7635e-01, -4.6192e-02,  1.7915e-01,\n",
       "         -1.8538e-01, -3.8840e-01,  3.4268e-01, -2.9121e-01, -4.7625e-01,\n",
       "         -6.8475e-02, -5.7472e-01,  2.6096e-01, -4.2949e-01, -3.2554e-01,\n",
       "         -5.0015e-01, -1.4221e-01, -8.5587e-02,  4.2519e-01,  1.4008e-01,\n",
       "         -3.1073e-01, -3.1307e-01,  1.0510e+00,  8.8331e-02, -4.9494e-01,\n",
       "          6.0144e-01,  3.4363e-01,  5.4549e-01, -1.6658e-02,  1.2222e-01,\n",
       "         -5.8815e-01,  3.5632e-01, -9.1474e-02, -5.4051e-01, -1.3024e-01,\n",
       "         -7.6787e-02,  5.9588e-01,  8.9727e-01, -1.6981e-01,  3.7350e-01,\n",
       "         -3.5145e-01, -2.4041e-01, -2.1279e-01, -5.1146e-01, -1.6454e-01,\n",
       "         -2.0483e-01, -2.0656e-02, -2.4655e-01,  3.4366e-01,  4.6987e-01,\n",
       "          2.4993e-01, -1.3211e-01, -3.3371e-01,  3.3258e-01,  3.7230e-01,\n",
       "         -5.4139e-01, -2.5593e-01,  4.4508e-02,  1.7162e-01, -7.1350e-01,\n",
       "          5.9075e-02,  9.9024e-02, -5.0001e-01, -7.6013e-02,  5.0818e-01,\n",
       "         -4.2799e-01,  1.4798e-01,  2.2100e-01,  8.9718e-01,  1.1130e-01,\n",
       "         -4.2691e-01,  1.2874e-02,  5.9093e-01,  1.9861e-01,  2.6063e-01,\n",
       "          7.2480e-02,  1.2343e+00,  1.6601e-01,  3.8327e-01, -7.0719e-02,\n",
       "         -8.1573e-02, -4.2009e-01, -7.8574e-01,  1.3621e-01,  3.9037e-01,\n",
       "         -8.5551e-02, -1.7139e-01, -3.7761e-01, -1.5451e-01,  1.3978e-01,\n",
       "          3.8662e-01, -1.7156e-01, -1.6370e-01,  4.2722e-01,  3.1686e-01,\n",
       "         -1.7100e-01, -2.6352e-02, -1.3597e-01,  4.4044e-02,  3.2884e-01,\n",
       "          2.6529e-02,  3.9625e-01, -1.8584e-01,  6.0466e-01,  3.3808e-03,\n",
       "         -6.1281e-01,  2.3222e-01, -4.8299e-01,  9.2179e-02,  3.8622e-01,\n",
       "          6.8952e-02, -4.4161e-01, -5.1834e-01, -6.5108e-01, -2.3758e-01,\n",
       "         -9.6368e-01,  3.1340e-01, -2.3314e-02, -4.2846e-01,  7.1849e-02,\n",
       "         -8.2422e-02,  2.4027e-01,  3.8486e-01, -5.0832e-01, -2.3980e-02,\n",
       "         -6.1238e-02, -3.2323e-02,  1.7635e-02, -3.5658e-01, -1.5851e-02,\n",
       "         -8.0537e-02,  4.2528e-02,  4.2610e-01,  2.8151e-01,  1.3655e-01,\n",
       "         -4.3204e-01, -1.3383e-01,  1.7406e-01,  7.0629e-01, -4.1053e-01,\n",
       "          2.3515e-02, -3.8374e-01,  1.7295e-01, -3.6867e-01,  1.6599e-01,\n",
       "          1.8400e-03, -3.4009e-01, -3.2741e-01,  4.8189e-02, -4.9594e-01,\n",
       "          2.3244e-01, -2.0434e-02, -1.1680e-01, -4.9149e-02, -3.0065e-02,\n",
       "         -3.9962e-02, -2.8128e-01,  1.5899e-01, -7.2783e-01, -3.8487e-01,\n",
       "          5.8431e-01,  2.1821e-01, -5.5564e-02,  1.1987e-01,  3.9414e-01,\n",
       "         -2.3777e-01, -1.3706e-01,  3.3407e-01, -6.5832e-01,  4.2695e-01,\n",
       "         -3.0942e-01,  3.6616e-01, -5.1648e-01,  7.2850e-02, -1.8037e-01,\n",
       "          2.5126e-01,  6.6585e-02,  5.2925e-02, -3.4348e-01, -3.9435e-01,\n",
       "          5.3978e-01,  9.0351e-02,  6.0725e-01, -2.2461e-01, -7.0742e-01,\n",
       "          4.6085e-01,  2.5949e-01,  3.2282e-01, -4.7003e-01, -6.7278e-01,\n",
       "         -6.6581e-01,  4.1323e-01,  8.4151e-03, -9.4121e-02, -2.3826e-01,\n",
       "          1.2272e-01, -6.3405e-01,  9.4701e-02,  3.0920e-01,  1.2868e+00,\n",
       "          3.5165e-01,  5.3980e-02, -4.1831e-01,  8.4949e-02, -3.7321e-01,\n",
       "         -6.0533e-02,  1.1541e-01, -4.1182e-01,  3.9471e-01, -3.6711e-01,\n",
       "          4.1430e-03, -2.1417e-02,  1.8770e-01,  1.8656e-01, -2.1484e-01,\n",
       "         -6.5784e-01, -1.7438e-02, -2.7840e-01, -1.8780e-01, -1.2814e+00,\n",
       "         -1.7170e-02,  4.0006e-01,  3.7599e-01, -2.9838e-01, -2.7034e-01,\n",
       "         -3.7610e-01,  2.5193e-01,  3.1715e-02,  5.1553e-01, -3.1188e-01,\n",
       "          4.0139e-02,  1.3537e-01,  4.9715e-02, -2.8977e-01,  1.8252e-01,\n",
       "         -1.5858e-01, -1.4596e-01,  1.0968e-01,  3.7097e-01, -4.4918e-02,\n",
       "          3.7617e-02, -5.6315e-01, -1.6630e-01, -5.9405e-03, -4.2766e-01,\n",
       "          1.6261e-01, -4.7103e-01,  5.3045e-01,  3.1739e-01, -5.6727e-01,\n",
       "          1.1124e-01,  1.1913e-01, -5.4574e-01,  8.6622e-02, -1.9828e-01,\n",
       "         -5.7538e-01,  9.4373e-01, -3.6943e-01, -8.3647e-01, -1.6718e-01,\n",
       "          1.5332e-01, -2.3330e-03, -1.8646e-01, -4.5878e-02,  6.5785e-02,\n",
       "         -1.4282e-01, -3.5683e-02, -1.0669e-01, -4.7360e-01,  1.9161e-01,\n",
       "          3.0857e-01,  2.4511e-01, -4.1980e-02, -3.4871e-02,  1.4818e-01,\n",
       "          2.6266e-01,  1.4415e-02,  3.9929e-01, -1.0245e-01,  8.9217e-02,\n",
       "         -6.8477e-01,  2.0326e-01,  3.5097e-01, -3.0381e-02, -2.4810e-01,\n",
       "         -7.9340e-01,  6.9090e-02,  3.8833e-01, -5.9009e-01,  3.9345e-01,\n",
       "         -2.6523e-02,  1.0122e-01, -5.7157e-02,  6.4440e-01, -3.0423e-01,\n",
       "         -1.0609e+00,  9.6705e-02, -1.0278e-01, -3.3288e-01,  4.2130e-01,\n",
       "          4.1408e-01, -4.2056e-01,  3.3658e-01, -3.4155e-01, -2.3900e-01,\n",
       "          1.2709e-01, -2.5660e-01,  2.6878e-01,  1.8024e-01,  3.3578e-02,\n",
       "         -1.2905e+00,  2.1314e-01,  2.6030e-02,  6.5118e-01,  4.5845e-03,\n",
       "         -2.8391e-01,  7.0053e-01, -1.2858e-01,  4.5035e-01, -3.7141e-01,\n",
       "         -1.5577e-01,  5.3307e-02, -3.3887e-01,  3.0630e-01, -1.2879e-01,\n",
       "         -2.4339e-01, -6.4226e-02, -1.4494e-01, -4.2014e-01,  4.7773e-01,\n",
       "         -5.0867e-01,  7.3546e-01,  1.4808e-01,  7.5217e-01, -2.4310e-02,\n",
       "         -5.9383e-01, -2.5199e-01, -2.4964e-01,  6.5989e-01, -5.8955e-01,\n",
       "          6.7143e-01,  1.3278e-01, -1.1592e-01,  5.6240e-01,  2.2257e-01,\n",
       "         -2.0130e-01, -3.9204e-01,  4.5532e-01,  3.5263e-01, -1.0936e-01,\n",
       "          6.9373e-02,  1.0742e+00, -1.9249e-01,  1.7510e-01,  8.3675e-02,\n",
       "         -8.5794e-02, -1.2888e-01, -4.1983e-01, -1.5382e-01,  2.6884e-01,\n",
       "         -1.5637e-01, -2.1280e-02,  9.6228e-01, -1.3665e-01,  4.0693e-01,\n",
       "          3.7013e-01,  3.3730e-01, -5.8172e-01,  9.3438e-02,  3.7142e-01,\n",
       "         -2.5127e-01, -3.2473e-01, -2.2518e-01, -6.8379e-02, -5.2630e-01,\n",
       "         -4.2603e-01,  1.4048e-01, -4.5158e-01,  4.8948e-01, -1.7288e-01,\n",
       "          1.4461e-01, -7.1906e-02, -4.6749e-02, -1.3074e+00, -7.9134e-01,\n",
       "         -2.2270e-01, -2.6262e-01,  4.3719e-01, -2.0157e-01,  2.7929e-01,\n",
       "          5.1742e-02,  7.5297e-01,  1.0445e-01,  4.6885e-01,  9.9330e-03,\n",
       "         -9.0373e-01, -5.9174e-01,  7.4086e-01, -5.6750e-01, -2.8507e-01,\n",
       "         -4.9451e-01,  6.5027e-01, -3.4019e-01, -1.3773e-01, -1.4028e-01,\n",
       "          1.7431e-01, -8.1873e-01, -4.6955e-01, -7.8405e-01,  1.6442e-02,\n",
       "          1.1609e-02,  2.0680e-01, -7.3816e-02,  9.9574e-02,  3.7054e-01,\n",
       "         -2.4469e-01, -6.8693e-01,  5.5905e-01, -5.3954e-01, -2.0823e-01,\n",
       "          2.8458e-01, -2.7733e-01,  8.0416e-01,  1.6081e-01,  1.0803e-02,\n",
       "         -4.8976e-01,  3.1387e-01,  3.1929e-01, -2.4932e-01,  1.9951e-01,\n",
       "         -1.5931e-02,  1.0601e-01, -1.4614e-01,  1.4355e-01, -2.5614e-01,\n",
       "          4.4750e-01,  6.2125e-01,  6.4609e-02, -3.3545e-01, -2.6755e-01,\n",
       "          4.1232e-01, -9.1879e-02, -5.7410e-01,  1.0965e-01,  2.9785e-01,\n",
       "          5.3075e-02, -4.3234e-01,  7.3832e-02, -2.8927e-01, -2.1075e-03,\n",
       "         -5.2950e-01, -4.1716e-01,  1.1126e+00, -4.2789e-01, -8.9019e-02,\n",
       "         -6.3761e-01, -1.4532e-01, -8.1605e-01,  1.0747e-01,  2.0020e-01,\n",
       "         -6.5888e-01,  3.9017e-01,  2.3992e-01,  3.1278e-01,  3.4015e-01,\n",
       "         -7.9547e-02,  5.9885e-01, -3.2118e-01, -3.3940e-01,  8.8732e-02,\n",
       "          9.7010e-02, -2.0979e-01, -2.8051e-02, -5.2145e-01,  3.9902e-01,\n",
       "         -4.4792e-01, -1.2889e-01,  1.7493e-01,  4.2241e-01, -4.6196e-01,\n",
       "          3.3498e-01, -2.9298e-01,  3.8058e-01,  3.1709e-01, -6.2564e-02,\n",
       "          3.6339e-01,  5.7660e-01,  4.1374e-01, -7.4347e-01, -2.6997e-01,\n",
       "          1.8204e-01,  1.3370e-01,  5.7200e-01,  2.0620e-01, -2.7648e-01,\n",
       "          3.2718e-02,  4.3439e-01,  1.9216e-01,  2.9547e-01, -2.7588e-01,\n",
       "          4.4889e-01,  2.4081e-01,  2.1337e-01,  2.1938e-01,  2.7769e-01,\n",
       "          4.9240e-01, -4.1537e-01, -3.9955e-01,  5.2807e-01,  3.4413e-01,\n",
       "         -5.2094e-01,  4.3800e-01,  1.4929e-01, -2.4409e-01,  2.0592e-01,\n",
       "         -1.3058e-01,  3.2167e-01,  1.3646e-01,  1.0591e-01, -9.1912e-01,\n",
       "         -2.1139e-01, -4.9956e-01,  6.0348e-01,  5.5196e-01, -2.1374e-01,\n",
       "          5.9726e-02,  1.2704e-02, -1.6886e-01, -1.6432e-01,  2.2830e-01,\n",
       "          8.3140e-03,  1.1884e+00,  1.9844e-01, -4.0456e-01,  1.0431e-01,\n",
       "         -3.0527e-01, -4.2828e-01,  8.9699e-02,  2.6789e-01,  2.9923e-01,\n",
       "         -2.8315e-01,  8.4890e-02, -9.4908e-02, -2.2777e-01, -5.1843e-02,\n",
       "         -9.2675e-02, -4.9686e-01, -4.9520e-01, -2.3534e-01,  3.0426e-01,\n",
       "         -8.4192e-02, -1.7818e-01,  5.5728e-02, -4.2451e-01, -4.2576e-01,\n",
       "         -3.2628e-01,  1.3096e-01, -3.5070e-01, -3.4167e-01, -1.1744e-01,\n",
       "         -1.8592e-01,  4.3619e-01, -1.1934e-02, -5.4876e-02, -1.7371e-02,\n",
       "          5.2603e-01,  1.2449e-01,  1.3093e-01,  4.3072e-01, -5.5198e-01,\n",
       "          2.1718e-01, -4.9922e-01, -3.0766e-01,  4.4481e-01,  1.8689e-01,\n",
       "          1.9628e-01, -7.2532e-01,  9.1787e-02, -3.8897e-01, -5.1246e-01,\n",
       "         -2.7159e-01, -6.7985e-02, -4.3572e-02, -4.0272e-01,  1.2658e-01,\n",
       "          1.0735e-01, -3.3746e-01, -2.9048e-01, -7.7949e-02, -4.3211e-01,\n",
       "          5.0689e-01, -3.5037e-01, -9.1113e-02, -1.7829e-01, -3.8580e-03,\n",
       "          5.3200e-01,  1.3960e-01, -4.7794e-01, -5.9988e-01, -5.2929e-01,\n",
       "          6.5678e+00,  3.0806e-01, -2.8323e-01,  3.2763e-01,  6.0088e-02,\n",
       "         -3.7939e-01,  3.9125e-01,  7.0756e-02,  2.0435e-01, -1.2179e-01,\n",
       "         -1.5038e-01,  5.8899e-01,  2.7591e-01, -2.0843e-02,  2.9441e-01,\n",
       "         -1.2256e-01,  4.5550e-01, -2.0198e-01, -1.4284e-01,  4.0438e-02,\n",
       "          3.2905e-01, -7.7685e-01, -3.2765e-02, -4.2777e-02,  1.2453e+00,\n",
       "          5.9947e-02, -3.9969e-01, -1.3064e-01, -3.3880e-01, -3.2387e-01,\n",
       "         -3.6148e-01, -3.6197e-01,  1.9931e-01,  2.4010e-01, -1.9317e-02,\n",
       "         -1.4485e-01, -4.6932e-02, -1.6625e-01,  6.3885e-01, -2.1397e-01,\n",
       "          5.4685e-01, -4.4169e-01, -5.8277e-01,  2.8556e-02,  2.6409e-01,\n",
       "          5.3934e-01,  5.1849e-02,  1.2033e-01, -2.0654e-01, -4.1706e-02,\n",
       "         -4.6221e-01,  1.4001e-01, -3.5360e-03, -1.6650e-01,  5.7189e-01,\n",
       "         -7.1926e-01, -8.2084e-01, -4.9682e-02, -2.9142e-01,  1.6361e-01,\n",
       "         -1.4988e-01,  2.8942e+00, -2.8692e-03,  8.0727e-01, -3.8448e-01,\n",
       "          3.1747e-01,  2.5303e-01,  4.0171e-01, -7.2013e-02, -3.8530e-01,\n",
       "          3.8307e-01,  2.0430e-01, -1.9834e-01]], grad_fn=<MeanBackward2>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_outputs.mean(1)  # 词向量平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4671,  0.2140,  0.4882, -0.2431, -0.5351, -0.7403,  0.1061,  0.8377,\n",
       "        -0.6703,  0.5808,  1.6504, -0.1127, -0.5356,  0.2803, -0.0265,  0.3146,\n",
       "        -0.6757,  0.2130,  1.3798, -0.2864, -0.7646,  0.0787, -0.8409,  0.6722,\n",
       "        -0.7090, -0.0265], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_outputs[0, :26, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0429, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_outputs[0, :26, 0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.3008, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_outputs[0, :26, 1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 torch.Size([21128, 768])\n",
      "2 torch.Size([512, 768])\n",
      "3 torch.Size([2, 768])\n",
      "4 torch.Size([768])\n",
      "5 torch.Size([768])\n",
      "6 torch.Size([768, 768])\n",
      "7 torch.Size([768])\n",
      "8 torch.Size([768, 768])\n",
      "9 torch.Size([768])\n",
      "10 torch.Size([768, 768])\n",
      "11 torch.Size([768])\n",
      "12 torch.Size([768, 768])\n",
      "13 torch.Size([768])\n",
      "14 torch.Size([768])\n",
      "15 torch.Size([768])\n",
      "16 torch.Size([3072, 768])\n",
      "17 torch.Size([3072])\n",
      "18 torch.Size([768, 3072])\n",
      "19 torch.Size([768])\n",
      "20 torch.Size([768])\n",
      "21 torch.Size([768])\n",
      "22 torch.Size([768, 768])\n",
      "23 torch.Size([768])\n",
      "24 torch.Size([768, 768])\n",
      "25 torch.Size([768])\n",
      "26 torch.Size([768, 768])\n",
      "27 torch.Size([768])\n",
      "28 torch.Size([768, 768])\n",
      "29 torch.Size([768])\n",
      "30 torch.Size([768])\n",
      "31 torch.Size([768])\n",
      "32 torch.Size([3072, 768])\n",
      "33 torch.Size([3072])\n",
      "34 torch.Size([768, 3072])\n",
      "35 torch.Size([768])\n",
      "36 torch.Size([768])\n",
      "37 torch.Size([768])\n",
      "38 torch.Size([768, 768])\n",
      "39 torch.Size([768])\n",
      "40 torch.Size([768, 768])\n",
      "41 torch.Size([768])\n",
      "42 torch.Size([768, 768])\n",
      "43 torch.Size([768])\n",
      "44 torch.Size([768, 768])\n",
      "45 torch.Size([768])\n",
      "46 torch.Size([768])\n",
      "47 torch.Size([768])\n",
      "48 torch.Size([3072, 768])\n",
      "49 torch.Size([3072])\n",
      "50 torch.Size([768, 3072])\n",
      "51 torch.Size([768])\n",
      "52 torch.Size([768])\n",
      "53 torch.Size([768])\n",
      "54 torch.Size([768, 768])\n",
      "55 torch.Size([768])\n",
      "56 torch.Size([768, 768])\n",
      "57 torch.Size([768])\n",
      "58 torch.Size([768, 768])\n",
      "59 torch.Size([768])\n",
      "60 torch.Size([768, 768])\n",
      "61 torch.Size([768])\n",
      "62 torch.Size([768])\n",
      "63 torch.Size([768])\n",
      "64 torch.Size([3072, 768])\n",
      "65 torch.Size([3072])\n",
      "66 torch.Size([768, 3072])\n",
      "67 torch.Size([768])\n",
      "68 torch.Size([768])\n",
      "69 torch.Size([768])\n",
      "70 torch.Size([768, 768])\n",
      "71 torch.Size([768])\n",
      "72 torch.Size([768, 768])\n",
      "73 torch.Size([768])\n",
      "74 torch.Size([768, 768])\n",
      "75 torch.Size([768])\n",
      "76 torch.Size([768, 768])\n",
      "77 torch.Size([768])\n",
      "78 torch.Size([768])\n",
      "79 torch.Size([768])\n",
      "80 torch.Size([3072, 768])\n",
      "81 torch.Size([3072])\n",
      "82 torch.Size([768, 3072])\n",
      "83 torch.Size([768])\n",
      "84 torch.Size([768])\n",
      "85 torch.Size([768])\n",
      "86 torch.Size([768, 768])\n",
      "87 torch.Size([768])\n",
      "88 torch.Size([768, 768])\n",
      "89 torch.Size([768])\n",
      "90 torch.Size([768, 768])\n",
      "91 torch.Size([768])\n",
      "92 torch.Size([768, 768])\n",
      "93 torch.Size([768])\n",
      "94 torch.Size([768])\n",
      "95 torch.Size([768])\n",
      "96 torch.Size([3072, 768])\n",
      "97 torch.Size([3072])\n",
      "98 torch.Size([768, 3072])\n",
      "99 torch.Size([768])\n",
      "100 torch.Size([768])\n",
      "101 torch.Size([768])\n",
      "102 torch.Size([768, 768])\n",
      "103 torch.Size([768])\n",
      "104 torch.Size([768, 768])\n",
      "105 torch.Size([768])\n",
      "106 torch.Size([768, 768])\n",
      "107 torch.Size([768])\n",
      "108 torch.Size([768, 768])\n",
      "109 torch.Size([768])\n",
      "110 torch.Size([768])\n",
      "111 torch.Size([768])\n",
      "112 torch.Size([3072, 768])\n",
      "113 torch.Size([3072])\n",
      "114 torch.Size([768, 3072])\n",
      "115 torch.Size([768])\n",
      "116 torch.Size([768])\n",
      "117 torch.Size([768])\n",
      "118 torch.Size([768, 768])\n",
      "119 torch.Size([768])\n",
      "120 torch.Size([768, 768])\n",
      "121 torch.Size([768])\n",
      "122 torch.Size([768, 768])\n",
      "123 torch.Size([768])\n",
      "124 torch.Size([768, 768])\n",
      "125 torch.Size([768])\n",
      "126 torch.Size([768])\n",
      "127 torch.Size([768])\n",
      "128 torch.Size([3072, 768])\n",
      "129 torch.Size([3072])\n",
      "130 torch.Size([768, 3072])\n",
      "131 torch.Size([768])\n",
      "132 torch.Size([768])\n",
      "133 torch.Size([768])\n",
      "134 torch.Size([768, 768])\n",
      "135 torch.Size([768])\n",
      "136 torch.Size([768, 768])\n",
      "137 torch.Size([768])\n",
      "138 torch.Size([768, 768])\n",
      "139 torch.Size([768])\n",
      "140 torch.Size([768, 768])\n",
      "141 torch.Size([768])\n",
      "142 torch.Size([768])\n",
      "143 torch.Size([768])\n",
      "144 torch.Size([3072, 768])\n",
      "145 torch.Size([3072])\n",
      "146 torch.Size([768, 3072])\n",
      "147 torch.Size([768])\n",
      "148 torch.Size([768])\n",
      "149 torch.Size([768])\n",
      "150 torch.Size([768, 768])\n",
      "151 torch.Size([768])\n",
      "152 torch.Size([768, 768])\n",
      "153 torch.Size([768])\n",
      "154 torch.Size([768, 768])\n",
      "155 torch.Size([768])\n",
      "156 torch.Size([768, 768])\n",
      "157 torch.Size([768])\n",
      "158 torch.Size([768])\n",
      "159 torch.Size([768])\n",
      "160 torch.Size([3072, 768])\n",
      "161 torch.Size([3072])\n",
      "162 torch.Size([768, 3072])\n",
      "163 torch.Size([768])\n",
      "164 torch.Size([768])\n",
      "165 torch.Size([768])\n",
      "166 torch.Size([768, 768])\n",
      "167 torch.Size([768])\n",
      "168 torch.Size([768, 768])\n",
      "169 torch.Size([768])\n",
      "170 torch.Size([768, 768])\n",
      "171 torch.Size([768])\n",
      "172 torch.Size([768, 768])\n",
      "173 torch.Size([768])\n",
      "174 torch.Size([768])\n",
      "175 torch.Size([768])\n",
      "176 torch.Size([3072, 768])\n",
      "177 torch.Size([3072])\n",
      "178 torch.Size([768, 3072])\n",
      "179 torch.Size([768])\n",
      "180 torch.Size([768])\n",
      "181 torch.Size([768])\n",
      "182 torch.Size([768, 768])\n",
      "183 torch.Size([768])\n",
      "184 torch.Size([768, 768])\n",
      "185 torch.Size([768])\n",
      "186 torch.Size([768, 768])\n",
      "187 torch.Size([768])\n",
      "188 torch.Size([768, 768])\n",
      "189 torch.Size([768])\n",
      "190 torch.Size([768])\n",
      "191 torch.Size([768])\n",
      "192 torch.Size([3072, 768])\n",
      "193 torch.Size([3072])\n",
      "194 torch.Size([768, 3072])\n",
      "195 torch.Size([768])\n",
      "196 torch.Size([768])\n",
      "197 torch.Size([768])\n",
      "198 torch.Size([768, 768])\n",
      "199 torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "for idx, x in enumerate(model.parameters(),1):\n",
    "    print(idx, x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.position_ids\n",
      "embeddings.word_embeddings.weight\n",
      "embeddings.position_embeddings.weight\n",
      "embeddings.token_type_embeddings.weight\n",
      "embeddings.LayerNorm.weight\n",
      "embeddings.LayerNorm.bias\n",
      "encoder.layer.0.attention.self.query.weight\n",
      "encoder.layer.0.attention.self.query.bias\n",
      "encoder.layer.0.attention.self.key.weight\n",
      "encoder.layer.0.attention.self.key.bias\n",
      "encoder.layer.0.attention.self.value.weight\n",
      "encoder.layer.0.attention.self.value.bias\n",
      "encoder.layer.0.attention.output.dense.weight\n",
      "encoder.layer.0.attention.output.dense.bias\n",
      "encoder.layer.0.attention.output.LayerNorm.weight\n",
      "encoder.layer.0.attention.output.LayerNorm.bias\n",
      "encoder.layer.0.intermediate.dense.weight\n",
      "encoder.layer.0.intermediate.dense.bias\n",
      "encoder.layer.0.output.dense.weight\n",
      "encoder.layer.0.output.dense.bias\n",
      "encoder.layer.0.output.LayerNorm.weight\n",
      "encoder.layer.0.output.LayerNorm.bias\n",
      "encoder.layer.1.attention.self.query.weight\n",
      "encoder.layer.1.attention.self.query.bias\n",
      "encoder.layer.1.attention.self.key.weight\n",
      "encoder.layer.1.attention.self.key.bias\n",
      "encoder.layer.1.attention.self.value.weight\n",
      "encoder.layer.1.attention.self.value.bias\n",
      "encoder.layer.1.attention.output.dense.weight\n",
      "encoder.layer.1.attention.output.dense.bias\n",
      "encoder.layer.1.attention.output.LayerNorm.weight\n",
      "encoder.layer.1.attention.output.LayerNorm.bias\n",
      "encoder.layer.1.intermediate.dense.weight\n",
      "encoder.layer.1.intermediate.dense.bias\n",
      "encoder.layer.1.output.dense.weight\n",
      "encoder.layer.1.output.dense.bias\n",
      "encoder.layer.1.output.LayerNorm.weight\n",
      "encoder.layer.1.output.LayerNorm.bias\n",
      "encoder.layer.2.attention.self.query.weight\n",
      "encoder.layer.2.attention.self.query.bias\n",
      "encoder.layer.2.attention.self.key.weight\n",
      "encoder.layer.2.attention.self.key.bias\n",
      "encoder.layer.2.attention.self.value.weight\n",
      "encoder.layer.2.attention.self.value.bias\n",
      "encoder.layer.2.attention.output.dense.weight\n",
      "encoder.layer.2.attention.output.dense.bias\n",
      "encoder.layer.2.attention.output.LayerNorm.weight\n",
      "encoder.layer.2.attention.output.LayerNorm.bias\n",
      "encoder.layer.2.intermediate.dense.weight\n",
      "encoder.layer.2.intermediate.dense.bias\n",
      "encoder.layer.2.output.dense.weight\n",
      "encoder.layer.2.output.dense.bias\n",
      "encoder.layer.2.output.LayerNorm.weight\n",
      "encoder.layer.2.output.LayerNorm.bias\n",
      "encoder.layer.3.attention.self.query.weight\n",
      "encoder.layer.3.attention.self.query.bias\n",
      "encoder.layer.3.attention.self.key.weight\n",
      "encoder.layer.3.attention.self.key.bias\n",
      "encoder.layer.3.attention.self.value.weight\n",
      "encoder.layer.3.attention.self.value.bias\n",
      "encoder.layer.3.attention.output.dense.weight\n",
      "encoder.layer.3.attention.output.dense.bias\n",
      "encoder.layer.3.attention.output.LayerNorm.weight\n",
      "encoder.layer.3.attention.output.LayerNorm.bias\n",
      "encoder.layer.3.intermediate.dense.weight\n",
      "encoder.layer.3.intermediate.dense.bias\n",
      "encoder.layer.3.output.dense.weight\n",
      "encoder.layer.3.output.dense.bias\n",
      "encoder.layer.3.output.LayerNorm.weight\n",
      "encoder.layer.3.output.LayerNorm.bias\n",
      "encoder.layer.4.attention.self.query.weight\n",
      "encoder.layer.4.attention.self.query.bias\n",
      "encoder.layer.4.attention.self.key.weight\n",
      "encoder.layer.4.attention.self.key.bias\n",
      "encoder.layer.4.attention.self.value.weight\n",
      "encoder.layer.4.attention.self.value.bias\n",
      "encoder.layer.4.attention.output.dense.weight\n",
      "encoder.layer.4.attention.output.dense.bias\n",
      "encoder.layer.4.attention.output.LayerNorm.weight\n",
      "encoder.layer.4.attention.output.LayerNorm.bias\n",
      "encoder.layer.4.intermediate.dense.weight\n",
      "encoder.layer.4.intermediate.dense.bias\n",
      "encoder.layer.4.output.dense.weight\n",
      "encoder.layer.4.output.dense.bias\n",
      "encoder.layer.4.output.LayerNorm.weight\n",
      "encoder.layer.4.output.LayerNorm.bias\n",
      "encoder.layer.5.attention.self.query.weight\n",
      "encoder.layer.5.attention.self.query.bias\n",
      "encoder.layer.5.attention.self.key.weight\n",
      "encoder.layer.5.attention.self.key.bias\n",
      "encoder.layer.5.attention.self.value.weight\n",
      "encoder.layer.5.attention.self.value.bias\n",
      "encoder.layer.5.attention.output.dense.weight\n",
      "encoder.layer.5.attention.output.dense.bias\n",
      "encoder.layer.5.attention.output.LayerNorm.weight\n",
      "encoder.layer.5.attention.output.LayerNorm.bias\n",
      "encoder.layer.5.intermediate.dense.weight\n",
      "encoder.layer.5.intermediate.dense.bias\n",
      "encoder.layer.5.output.dense.weight\n",
      "encoder.layer.5.output.dense.bias\n",
      "encoder.layer.5.output.LayerNorm.weight\n",
      "encoder.layer.5.output.LayerNorm.bias\n",
      "encoder.layer.6.attention.self.query.weight\n",
      "encoder.layer.6.attention.self.query.bias\n",
      "encoder.layer.6.attention.self.key.weight\n",
      "encoder.layer.6.attention.self.key.bias\n",
      "encoder.layer.6.attention.self.value.weight\n",
      "encoder.layer.6.attention.self.value.bias\n",
      "encoder.layer.6.attention.output.dense.weight\n",
      "encoder.layer.6.attention.output.dense.bias\n",
      "encoder.layer.6.attention.output.LayerNorm.weight\n",
      "encoder.layer.6.attention.output.LayerNorm.bias\n",
      "encoder.layer.6.intermediate.dense.weight\n",
      "encoder.layer.6.intermediate.dense.bias\n",
      "encoder.layer.6.output.dense.weight\n",
      "encoder.layer.6.output.dense.bias\n",
      "encoder.layer.6.output.LayerNorm.weight\n",
      "encoder.layer.6.output.LayerNorm.bias\n",
      "encoder.layer.7.attention.self.query.weight\n",
      "encoder.layer.7.attention.self.query.bias\n",
      "encoder.layer.7.attention.self.key.weight\n",
      "encoder.layer.7.attention.self.key.bias\n",
      "encoder.layer.7.attention.self.value.weight\n",
      "encoder.layer.7.attention.self.value.bias\n",
      "encoder.layer.7.attention.output.dense.weight\n",
      "encoder.layer.7.attention.output.dense.bias\n",
      "encoder.layer.7.attention.output.LayerNorm.weight\n",
      "encoder.layer.7.attention.output.LayerNorm.bias\n",
      "encoder.layer.7.intermediate.dense.weight\n",
      "encoder.layer.7.intermediate.dense.bias\n",
      "encoder.layer.7.output.dense.weight\n",
      "encoder.layer.7.output.dense.bias\n",
      "encoder.layer.7.output.LayerNorm.weight\n",
      "encoder.layer.7.output.LayerNorm.bias\n",
      "encoder.layer.8.attention.self.query.weight\n",
      "encoder.layer.8.attention.self.query.bias\n",
      "encoder.layer.8.attention.self.key.weight\n",
      "encoder.layer.8.attention.self.key.bias\n",
      "encoder.layer.8.attention.self.value.weight\n",
      "encoder.layer.8.attention.self.value.bias\n",
      "encoder.layer.8.attention.output.dense.weight\n",
      "encoder.layer.8.attention.output.dense.bias\n",
      "encoder.layer.8.attention.output.LayerNorm.weight\n",
      "encoder.layer.8.attention.output.LayerNorm.bias\n",
      "encoder.layer.8.intermediate.dense.weight\n",
      "encoder.layer.8.intermediate.dense.bias\n",
      "encoder.layer.8.output.dense.weight\n",
      "encoder.layer.8.output.dense.bias\n",
      "encoder.layer.8.output.LayerNorm.weight\n",
      "encoder.layer.8.output.LayerNorm.bias\n",
      "encoder.layer.9.attention.self.query.weight\n",
      "encoder.layer.9.attention.self.query.bias\n",
      "encoder.layer.9.attention.self.key.weight\n",
      "encoder.layer.9.attention.self.key.bias\n",
      "encoder.layer.9.attention.self.value.weight\n",
      "encoder.layer.9.attention.self.value.bias\n",
      "encoder.layer.9.attention.output.dense.weight\n",
      "encoder.layer.9.attention.output.dense.bias\n",
      "encoder.layer.9.attention.output.LayerNorm.weight\n",
      "encoder.layer.9.attention.output.LayerNorm.bias\n",
      "encoder.layer.9.intermediate.dense.weight\n",
      "encoder.layer.9.intermediate.dense.bias\n",
      "encoder.layer.9.output.dense.weight\n",
      "encoder.layer.9.output.dense.bias\n",
      "encoder.layer.9.output.LayerNorm.weight\n",
      "encoder.layer.9.output.LayerNorm.bias\n",
      "encoder.layer.10.attention.self.query.weight\n",
      "encoder.layer.10.attention.self.query.bias\n",
      "encoder.layer.10.attention.self.key.weight\n",
      "encoder.layer.10.attention.self.key.bias\n",
      "encoder.layer.10.attention.self.value.weight\n",
      "encoder.layer.10.attention.self.value.bias\n",
      "encoder.layer.10.attention.output.dense.weight\n",
      "encoder.layer.10.attention.output.dense.bias\n",
      "encoder.layer.10.attention.output.LayerNorm.weight\n",
      "encoder.layer.10.attention.output.LayerNorm.bias\n",
      "encoder.layer.10.intermediate.dense.weight\n",
      "encoder.layer.10.intermediate.dense.bias\n",
      "encoder.layer.10.output.dense.weight\n",
      "encoder.layer.10.output.dense.bias\n",
      "encoder.layer.10.output.LayerNorm.weight\n",
      "encoder.layer.10.output.LayerNorm.bias\n",
      "encoder.layer.11.attention.self.query.weight\n",
      "encoder.layer.11.attention.self.query.bias\n",
      "encoder.layer.11.attention.self.key.weight\n",
      "encoder.layer.11.attention.self.key.bias\n",
      "encoder.layer.11.attention.self.value.weight\n",
      "encoder.layer.11.attention.self.value.bias\n",
      "encoder.layer.11.attention.output.dense.weight\n",
      "encoder.layer.11.attention.output.dense.bias\n",
      "encoder.layer.11.attention.output.LayerNorm.weight\n",
      "encoder.layer.11.attention.output.LayerNorm.bias\n",
      "encoder.layer.11.intermediate.dense.weight\n",
      "encoder.layer.11.intermediate.dense.bias\n",
      "encoder.layer.11.output.dense.weight\n",
      "encoder.layer.11.output.dense.bias\n",
      "encoder.layer.11.output.LayerNorm.weight\n",
      "encoder.layer.11.output.LayerNorm.bias\n",
      "pooler.dense.weight\n",
      "pooler.dense.bias\n"
     ]
    }
   ],
   "source": [
    "for x in model.state_dict():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers==3.1.0 -i https://mirrors.aliyun.com/pypi/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
