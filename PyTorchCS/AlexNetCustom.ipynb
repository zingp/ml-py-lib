{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, class_num):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # in=3, out=96, kernel_size=(11,11), inputs=(227,227,3),stride=4 -> (227-11+2*0)/4 +1 --> 55*55*96\n",
    "            nn.Conv2d(3, 96, (11, 11), stride=4),\n",
    "            nn.ReLU(inplace=True),  # inplace为True，将会改变输入的数据 ，否则不会改变原输入，只会产生新的输出。\n",
    "            nn.MaxPool2d((3,3), stride=2),    # (55-3)/2+1 = 27\n",
    "            # in=96, out=256, kerner=(5,5), inputs=(27,27,96) stride=1, padding=2->27*27*256\n",
    "            nn.Conv2d(96, 256, (5,5), padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d((3,3), stride=2),      # (27-3)/2 +1= 13*13*256\n",
    "            # in=256,out=384,kernel=(3,3), inputs=(13,13,256),padding=1\n",
    "            nn.Conv2d(256, 384, (3,3), padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, (3,3), padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, (3,3), padding=1),  # 13*13*256\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d((3,3), stride=2),  # 6*6*256 = 9216\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(6*6*256, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, class_num),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        #print(x.size())  #torch.Size([8, 256, 6, 6])  原本是[32,256, 6, 6],我这里有4个核\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accu(out, y):\n",
    "    _, pred = torch.max(out, 1)  # 最大概率，对应的类别\n",
    "    corr_num = pred.eq(y).sum()  # tensor 得取出结果\n",
    "    acc = corr_num.item() / y.shape[0]\n",
    "    return acc\n",
    "\n",
    "\n",
    "# 单epoch的训练\n",
    "def train(model, device, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    for x, y in train_loader:\n",
    "#         x = x.view(x.size(0), -1)    # reshape(x.size(0), 该维度压平)\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)               # 各个分类的概率\n",
    "        loss = criterion(out, y)\n",
    "        train_loss += loss\n",
    "        acc = accu(out, y)\n",
    "        train_acc += acc\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return train_loss / len(train_loader), train_acc / len(train_loader) \n",
    "\n",
    "# 测试\n",
    "def test(model, device, test_loader, criterion, optimizer):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    for x, y in test_loader:\n",
    "#         x = x.view(x.size(0), -1)    # reshape(x.size(0), 该维度压平)\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        out = model(x)               # 各个分类的概率\n",
    "        loss = criterion(out, y)\n",
    "        test_loss += loss\n",
    "        acc = accu(out, y)\n",
    "        test_acc += acc\n",
    "        loss.backward()\n",
    "    return test_loss/len(test_loader), test_acc/len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trans = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomCrop(32,padding=3),\n",
    "    transforms.Resize(227),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.49139968, 0.48215827, 0.44653124), (0.24703233, 0.24348505, 0.26158768))#参数mean和std来自于训练集，但是transform本身会在训练和评测的时候都会使用\n",
    "])\n",
    "\n",
    "data_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(227),\n",
    "    transforms.Normalize((0.49139968, 0.48215827, 0.44653124), (0.24703233, 0.24348505, 0.26158768))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_data = datasets.CIFAR10('data', train=True, download=True, transform=data_trans)\n",
    "test_data = datasets.CIFAR10('data', train=False, download=True, transform=data_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(len(train_data)*0.9)\n",
    "n_valid = len(train_data) - n_train\n",
    "train_datasets, valid_datasets = torch.utils.data.random_split(train_data, [n_train, n_valid])\n",
    "\n",
    "train_loader = DataLoader(train_datasets, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_datasets, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AlexNet(10)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "    model = model.to(device)\n",
    "else:\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save the best models to ./models/alexnet-sifar10.pth\n",
      "Epoch:1|Train Loss:2.303602457046509|Train Acc:0.09863628287135749|Val Loss:2.3033065795898438|Val Acc:0.09673566878980892\n",
      "Save the best models to ./models/alexnet-sifar10.pth\n",
      "Epoch:2|Train Loss:2.302884578704834|Train Acc:0.09899164889836531|Val Loss:2.3029096126556396|Val Acc:0.09673566878980892\n",
      "Save the best models to ./models/alexnet-sifar10.pth\n",
      "Epoch:3|Train Loss:2.302786350250244|Train Acc:0.09801439232409381|Val Loss:2.302788734436035|Val Acc:0.09852707006369427\n",
      "Save the best models to ./models/alexnet-sifar10.pth\n",
      "Epoch:4|Train Loss:2.3027760982513428|Train Acc:0.09614872068230278|Val Loss:2.3027725219726562|Val Acc:0.10051751592356688\n",
      "Save the best models to ./models/alexnet-sifar10.pth\n",
      "Epoch:5|Train Loss:2.3027961254119873|Train Acc:0.09768123667377399|Val Loss:2.302732229232788|Val Acc:0.09832802547770701\n",
      "Save the best models to ./models/alexnet-sifar10.pth\n",
      "Epoch:6|Train Loss:2.3027961254119873|Train Acc:0.09876954513148543|Val Loss:2.30275297164917|Val Acc:0.09315286624203821\n",
      "Save the best models to ./models/alexnet-sifar10.pth\n",
      "Epoch:7|Train Loss:2.302762508392334|Train Acc:0.09874733475479744|Val Loss:2.3027608394622803|Val Acc:0.09852707006369427\n",
      "Save the best models to ./models/alexnet-sifar10.pth\n",
      "Epoch:8|Train Loss:2.302805185317993|Train Acc:0.0947050461975835|Val Loss:2.3026859760284424|Val Acc:0.10330414012738853\n",
      "Save the best models to ./models/alexnet-sifar10.pth\n",
      "Epoch:9|Train Loss:2.3027520179748535|Train Acc:0.09868070362473348|Val Loss:2.3026466369628906|Val Acc:0.09832802547770701\n",
      "Save the best models to ./models/alexnet-sifar10.pth\n",
      "Epoch:10|Train Loss:2.302812099456787|Train Acc:0.09943585643212509|Val Loss:2.3027329444885254|Val Acc:0.09832802547770701\n"
     ]
    }
   ],
   "source": [
    "model_path = './models/alexnet-sifar10.pth'\n",
    "info = 'Epoch:{0}|Train Loss:{1}|Train Acc:{2}|Val Loss:{3}|Val Acc:{4}'\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train(model, device, train_loader, criterion, optimizer)\n",
    "    valid_loss, valid_acc = test(model, device, valid_loader, criterion, optimizer)\n",
    "    if valid_loss < float(\"inf\"):\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(\"Save the best models to {}\".format(model_path))\n",
    "    print(info.format(epoch+1, train_loss, train_acc, valid_loss, valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图像增强\n",
    "### torchvision.transforms 包括所有罔像增强的方法 。\n",
    "- Scale，对图片的尺度进行缩小和放大;\n",
    "- RandomRotation 对图片随机旋转；\n",
    "- CenterCrop，对图像正中心进行给定大小的裁剪;\n",
    "- RandomCrop，对图片进行给定大小的随机裁剪;\n",
    "- RandomHorizaontalFlip，对图片进行概率为0.5的随机水平翻转;\n",
    "- RandomSizedCrop，首先对图片进行随机尺寸的裁剪，然后对裁剪的图片进行一个随机比例的缩放，最后将图片变成给定的大小，这在Inception Net 中比较流行;\n",
    "- 最后一个是 Pad，对图片进行边界零填充。\n",
    "\n",
    "上面介绍了 PyTorch 内置的一些图像增强的方法，还有更多的增强方法见[transforms的二十二个方法](https://zhuanlan.zhihu.com/p/53367135)，可以使用OpenCV或者PIL等第二方图形库实现。在网络的训练中图像增强是一种常见、默认的做法，对多任务进行图像增强之后都能够在一定程度上提升任务的准确率。\n",
    "\n",
    "- 本篇幅参考[经典CNN网络 - AlexNet总结](https://juejin.im/post/5ad173b651882555731c8f9b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "?transforms.RandomRotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
