{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = 6785\n",
    "n_embed = 300\n",
    "num_filters = 250\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    '''Deep Pyramid Convolutional Neural Networks for Text Categorization'''\n",
    "    def __init__(self, n_vocab, n_embed, num_filters, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed, padding_idx=n_vocab - 1)\n",
    "        self.conv_region = nn.Conv2d(1, num_filters, (3, n_embed), stride=1)\n",
    "        self.conv = nn.Conv2d(num_filters, num_filters, (3, 1), stride=1)\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=(3, 1), stride=2)\n",
    "        self.padding1 = nn.ZeroPad2d((0, 0, 1, 1))  # top bottom\n",
    "        self.padding2 = nn.ZeroPad2d((0, 0, 0, 1))  # bottom\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(num_filters, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x[0]\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)       # [batch_size, 250, seq_len, 1]\n",
    "        x = self.conv_region(x)  # [batch_size, 250, seq_len-3+1, 1]\n",
    "\n",
    "        x = self.padding1(x)  # [batch_size, 250, seq_len, 1]\n",
    "        x = self.relu(x)\n",
    "        x = self.conv(x)      # [batch_size, 250, seq_len-3+1, 1]\n",
    "        x = self.padding1(x)  # [batch_size, 250, seq_len, 1]\n",
    "        x = self.relu(x)\n",
    "        x = self.conv(x)      # [batch_size, 250, seq_len-3+1, 1]\n",
    "        while x.size()[2] > 2:\n",
    "            x = self._block(x)\n",
    "        print(\"1----\", x.size())\n",
    "        x = x.squeeze()       # [batch_size, num_filters(250)]\n",
    "        print(\"2----\", x.size())\n",
    "        x = self.fc(x)\n",
    "        print(\"3----\", x.size())\n",
    "        return x\n",
    "\n",
    "    def _block(self, x):\n",
    "        x = self.padding2(x)\n",
    "        px = self.max_pool(x)\n",
    "\n",
    "        x = self.padding1(px)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv(x)\n",
    "\n",
    "        x = self.padding1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv(x)\n",
    "\n",
    "        x = x + px\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(n_vocab, n_embed, num_filters, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (embedding): Embedding(6785, 300, padding_idx=6784)\n",
       "  (conv_region): Conv2d(1, 250, kernel_size=(3, 300), stride=(1, 1))\n",
       "  (conv): Conv2d(250, 250, kernel_size=(3, 1), stride=(1, 1))\n",
       "  (max_pool): MaxPool2d(kernel_size=(3, 1), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (padding1): ZeroPad2d(padding=(0, 0, 1, 1), value=0.0)\n",
       "  (padding2): ZeroPad2d(padding=(0, 0, 0, 1), value=0.0)\n",
       "  (relu): ReLU()\n",
       "  (fc): Linear(in_features=250, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(x, seq_len), y\n",
    "# x = torch.LongTensor([_[0] for _ in datas]).to(self.device)\n",
    "# y = torch.LongTensor([_[1] for _ in datas]).to(self.device)\n",
    "\n",
    "# # pad前的长度(超过pad_size的设为pad_size)\n",
    "# seq_len = torch.LongTensor([_[2] for _ in datas]).to(self.device)\n",
    "# return (x, seq_len), y\n",
    "pad_size = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(128):\n",
    "    x = torch.randint(0, n_vocab, (pad_size,))\n",
    "    y = 1\n",
    "    seq_len = pad_size\n",
    "    data.append((x, y, seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.LongTensor([_[0].numpy() for _ in data])\n",
    "y = torch.LongTensor([_[1] for _ in data])\n",
    "seq_len = torch.LongTensor([_[2] for _ in data])\n",
    "batch = (x, seq_len), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = (x, seq_len), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[2641, 3142, 6038,  ..., 4882, 6404, 5904],\n",
       "          [ 674,  180, 3888,  ..., 5296, 6608, 4757],\n",
       "          [3255, 2628, 3481,  ...,  403, 3442, 3272],\n",
       "          ...,\n",
       "          [5632, 4497, 1870,  ..., 3487, 3319, 1152],\n",
       "          [6124, 3428, 3456,  ..., 1180,  198, 1373],\n",
       "          [1043, 2513, 2178,  ..., 6002,  426, 6406]]),\n",
       "  tensor([2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000,\n",
       "          2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000,\n",
       "          2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000,\n",
       "          2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000,\n",
       "          2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000,\n",
       "          2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000,\n",
       "          2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000,\n",
       "          2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000,\n",
       "          2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000,\n",
       "          2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000,\n",
       "          2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000])),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1---- torch.Size([128, 250, 1, 1])\n",
      "2---- torch.Size([128, 250])\n",
      "3---- torch.Size([128, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1484,  0.5910],\n",
       "        [-1.1393,  0.6055],\n",
       "        [-1.1429,  0.6078],\n",
       "        [-1.1623,  0.6407],\n",
       "        [-1.1470,  0.5874],\n",
       "        [-1.1105,  0.6037],\n",
       "        [-1.1328,  0.6101],\n",
       "        [-1.1027,  0.6200],\n",
       "        [-1.1198,  0.6108],\n",
       "        [-1.1542,  0.6378],\n",
       "        [-1.1362,  0.5857],\n",
       "        [-1.1055,  0.5995],\n",
       "        [-1.1283,  0.5863],\n",
       "        [-1.1505,  0.6202],\n",
       "        [-1.1410,  0.5996],\n",
       "        [-1.1273,  0.5883],\n",
       "        [-1.0928,  0.6062],\n",
       "        [-1.1119,  0.6081],\n",
       "        [-1.1025,  0.6386],\n",
       "        [-1.1303,  0.6089],\n",
       "        [-1.1290,  0.6132],\n",
       "        [-1.1077,  0.6144],\n",
       "        [-1.1063,  0.5967],\n",
       "        [-1.0977,  0.6082],\n",
       "        [-1.1522,  0.5888],\n",
       "        [-1.1152,  0.6270],\n",
       "        [-1.1378,  0.5977],\n",
       "        [-1.1371,  0.5990],\n",
       "        [-1.1483,  0.6291],\n",
       "        [-1.1017,  0.6201],\n",
       "        [-1.1417,  0.5767],\n",
       "        [-1.1428,  0.5843],\n",
       "        [-1.1030,  0.5875],\n",
       "        [-1.1252,  0.6312],\n",
       "        [-1.1298,  0.6594],\n",
       "        [-1.1188,  0.6178],\n",
       "        [-1.1285,  0.5964],\n",
       "        [-1.1001,  0.6004],\n",
       "        [-1.1334,  0.6197],\n",
       "        [-1.1327,  0.5885],\n",
       "        [-1.1432,  0.6138],\n",
       "        [-1.1344,  0.6114],\n",
       "        [-1.0963,  0.5917],\n",
       "        [-1.1180,  0.5928],\n",
       "        [-1.1688,  0.6199],\n",
       "        [-1.1614,  0.6119],\n",
       "        [-1.1401,  0.5890],\n",
       "        [-1.1478,  0.6084],\n",
       "        [-1.1245,  0.6316],\n",
       "        [-1.1392,  0.5824],\n",
       "        [-1.1477,  0.5890],\n",
       "        [-1.1203,  0.6081],\n",
       "        [-1.1584,  0.5912],\n",
       "        [-1.1146,  0.5894],\n",
       "        [-1.1050,  0.5627],\n",
       "        [-1.1106,  0.6182],\n",
       "        [-1.1433,  0.6296],\n",
       "        [-1.1570,  0.5678],\n",
       "        [-1.0994,  0.5840],\n",
       "        [-1.1142,  0.6332],\n",
       "        [-1.1072,  0.5943],\n",
       "        [-1.1414,  0.6657],\n",
       "        [-1.0937,  0.5979],\n",
       "        [-1.1253,  0.5896],\n",
       "        [-1.1128,  0.6075],\n",
       "        [-1.1259,  0.5996],\n",
       "        [-1.1157,  0.5931],\n",
       "        [-1.1171,  0.6002],\n",
       "        [-1.1321,  0.6034],\n",
       "        [-1.0986,  0.5989],\n",
       "        [-1.1020,  0.6186],\n",
       "        [-1.1153,  0.6137],\n",
       "        [-1.1157,  0.5946],\n",
       "        [-1.1350,  0.5874],\n",
       "        [-1.1071,  0.6047],\n",
       "        [-1.1112,  0.6308],\n",
       "        [-1.1087,  0.5910],\n",
       "        [-1.1258,  0.6312],\n",
       "        [-1.1476,  0.5774],\n",
       "        [-1.1228,  0.6068],\n",
       "        [-1.1245,  0.5966],\n",
       "        [-1.0785,  0.5883],\n",
       "        [-1.1038,  0.6214],\n",
       "        [-1.1582,  0.5992],\n",
       "        [-1.0865,  0.6094],\n",
       "        [-1.1199,  0.5531],\n",
       "        [-1.1487,  0.6364],\n",
       "        [-1.1001,  0.6330],\n",
       "        [-1.1330,  0.5911],\n",
       "        [-1.1051,  0.5724],\n",
       "        [-1.1516,  0.5769],\n",
       "        [-1.1051,  0.6048],\n",
       "        [-1.1190,  0.6197],\n",
       "        [-1.1353,  0.5641],\n",
       "        [-1.1067,  0.5622],\n",
       "        [-1.1158,  0.5806],\n",
       "        [-1.1132,  0.6293],\n",
       "        [-1.1061,  0.6328],\n",
       "        [-1.1444,  0.5846],\n",
       "        [-1.1215,  0.6281],\n",
       "        [-1.0861,  0.6139],\n",
       "        [-1.0972,  0.6147],\n",
       "        [-1.1291,  0.6124],\n",
       "        [-1.1568,  0.5737],\n",
       "        [-1.1363,  0.5943],\n",
       "        [-1.0981,  0.5930],\n",
       "        [-1.1259,  0.6293],\n",
       "        [-1.1203,  0.6186],\n",
       "        [-1.0948,  0.6258],\n",
       "        [-1.1462,  0.6202],\n",
       "        [-1.1019,  0.6260],\n",
       "        [-1.1079,  0.6492],\n",
       "        [-1.0943,  0.5747],\n",
       "        [-1.0988,  0.6227],\n",
       "        [-1.1151,  0.6202],\n",
       "        [-1.1006,  0.5853],\n",
       "        [-1.1018,  0.5910],\n",
       "        [-1.0765,  0.6151],\n",
       "        [-1.1523,  0.6110],\n",
       "        [-1.1026,  0.6006],\n",
       "        [-1.0884,  0.6044],\n",
       "        [-1.1289,  0.6210],\n",
       "        [-1.0801,  0.6026],\n",
       "        [-1.1308,  0.6206],\n",
       "        [-1.1441,  0.5791],\n",
       "        [-1.1076,  0.6243],\n",
       "        [-1.1171,  0.6116],\n",
       "        [-1.0960,  0.6029]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(new_data[0])\n",
    "# 1---- torch.Size([128, 250, 2, 1])\n",
    "# 2---- torch.Size([128, 250, 2])\n",
    "## 正常\n",
    "# 1---- torch.Size([128, 250, 1, 1])\n",
    "# 2---- torch.Size([128, 250])\n",
    "# 3---- torch.Size([128, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1500])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
