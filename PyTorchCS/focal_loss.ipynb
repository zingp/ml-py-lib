{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FocalLoss\n",
    "- $\\mathrm{FL}\\left(p_{\\mathrm{t}}\\right)=-\\left(1-p_{\\mathrm{t}}\\right)^{\\gamma} \\log \\left(p_{\\mathrm{t}}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, weight=None, reduction='mean', gamma=2, eps=1e-7):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.ce = torch.nn.CrossEntropyLoss(weight=weight, reduction=reduction)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logp = self.ce(input, target)\n",
    "        p = torch.exp(-logp)\n",
    "        loss = (1 - p) ** self.gamma * logp\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设词汇表的大小为3， 语料包含两个单词\"2 0\"\n",
    "y = [2, 0, 1, 1]\n",
    "# 假设模型对两个单词预测时，产生的logit分别是[2.0, -1.0, 3.0]和[1.0, 0.0, -0.5]\n",
    "y_logits = [[2.0, -1.0, 3.0], [1.0, 0.0, -0.5], [2.0, 1.0, -0.5], [1, 8, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5415)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(torch.tensor(y_logits), torch.tensor(y,dtype=torch.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1015)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FocalLoss(weight=torch.Tensor([0.25, 0.75, 0.75]), gamma=2)(torch.tensor(y_logits), torch.tensor(y,dtype=torch.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class focal_loss(nn.Module):\n",
    "    \"\"\"https://github.com/yatengLG/Focal-Loss-Pytorch/blob/master/Focal_Loss.py\"\"\"\n",
    "    def __init__(self, alpha=0.25, gamma=2, num_classes=3, size_average=True):\n",
    "        \"\"\"\n",
    "        focal_loss损失函数, -α(1-yi)**γ *ce_loss(xi,yi)\n",
    "        步骤详细的实现了 focal_loss损失函数.\n",
    "        :param alpha:  阿尔法α, 类别权重. 当α是列表时,为各类别权重,当α为常数时,类别权重为[α, 1-α, 1-α, ....],常用于 目标检测算法中抑制背景类 , retainnet中设置为0.25\n",
    "        :param gamma:  伽马γ, 难易分样本调节参数. retainnet中设置为2\n",
    "        :param num_classes:     类别数量\n",
    "        :param size_average:    损失计算方式,默认取均值\n",
    "        \"\"\"\n",
    "        super(focal_loss,self).__init__()\n",
    "        self.size_average = size_average\n",
    "        # α可以以list方式输入,size:[num_classes] 用于对不同类别精细地赋予权重\n",
    "        if isinstance(alpha, list):\n",
    "            assert len(alpha) == num_classes\n",
    "            self.alpha = torch.Tensor(alpha)\n",
    "        else:\n",
    "            assert alpha < 1   # 如果α为一个常数,则降低第一类的影响\n",
    "            self.alpha = torch.zeros(num_classes)\n",
    "            self.alpha[0] += alpha\n",
    "            # α 最终为 [ α, 1-α, 1-α, 1-α, 1-α, ...] size:[num_classes]\n",
    "            self.alpha[1:] += (1-alpha) \n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, preds, labels):\n",
    "        \"\"\"\n",
    "        focal_loss损失计算\n",
    "        :param preds:   预测类别. size:[B,N,C] or [B,C] 分别对应与检测与分类任务, B 批次, N检测框数, C类别数\n",
    "        :param labels:  实际类别. size:[B,N] or [B]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # assert preds.dim()==2 and labels.dim()==1\n",
    "        preds = preds.view(-1, preds.size(-1))\n",
    "        self.alpha = self.alpha.to(preds.device)\n",
    "        preds_logsoft = F.log_softmax(preds, dim=1) # log_softmax\n",
    "        preds_softmax = torch.exp(preds_logsoft)    # softmax\n",
    "        # 这部分实现nll_loss ( crossempty = log_softmax + nll )\n",
    "        preds_softmax = preds_softmax.gather(1, labels.view(-1,1))   \n",
    "        preds_logsoft = preds_logsoft.gather(1, labels.view(-1,1))\n",
    "        self.alpha = self.alpha.gather(0, labels.view(-1))\n",
    "        print(\"alpha\", self.alpha)\n",
    "        # torch.pow((1-preds_softmax), self.gamma) 为focal loss中 (1-pt)**γ\n",
    "        loss = -torch.mul(torch.pow((1 - preds_softmax), self.gamma), preds_logsoft)  \n",
    "        print(\"loss:\", loss)\n",
    "        loss = torch.mul(self.alpha, loss.t())\n",
    "        if self.size_average:\n",
    "            loss = loss.mean()\n",
    "        else:\n",
    "            loss = loss.sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl = focal_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha tensor([0.7500, 0.2500, 0.7500, 0.7500])\n",
      "loss: tensor([[2.5347e-02],\n",
      "        [6.4078e-02],\n",
      "        [7.6386e-01],\n",
      "        [3.8641e-08]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.1520)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fl(torch.tensor(y_logits), torch.tensor(y,dtype=torch.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 0, 1, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.tensor(y,dtype=torch.int64)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2500, 0.7500, 0.7500])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([0.2500, 0.7500, 0.75])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7500, 0.2500, 0.7500, 0.7500])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.gather(0, labels.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
