{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, class_num):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # in=3, out=96, kernel_size=(11,11), inputs=(227,227,3),stride=4 -> (227-11+2*0)/4 +1 --> 55*55*96\n",
    "            nn.Conv2d(3, 96, (11, 11), stride=4),\n",
    "            nn.ReLU(inplace=True),  # inplace为True，将会改变输入的数据 ，否则不会改变原输入，只会产生新的输出。\n",
    "            nn.MaxPool2d((3,3), stride=2),    # (55-3)/2+1 = 27\n",
    "            # in=96, out=256, kerner=(5,5), inputs=(27,27,96) stride=1, padding=2->27*27*256\n",
    "            nn.Conv2d(96, 256, (5,5), padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d((3,3), stride=2),      # (27-3)/2 +1= 13*13*256\n",
    "            # in=256,out=384,kernel=(3,3), inputs=(13,13,256),padding=1\n",
    "            nn.Conv2d(256, 384, (3,3), padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, (3,3), padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, (3,3), padding=1),  # 13*13*256\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d((3,3), stride=2),  # 6*6*256 = 9216\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(6*6*256, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, class_num),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        #print(x.size())  #torch.Size([8, 256, 6, 6])  原本是[32,256, 6, 6],我这里有4个核\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accu(out, y):\n",
    "    _, pred = torch.max(out, 1)  # 最大概率，对应的类别\n",
    "    corr_num = pred.eq(y).sum()  # tensor 得取出结果\n",
    "    acc = corr_num.item() / y.shape[0]\n",
    "    return acc\n",
    "\n",
    "# 单epoch的训练\n",
    "def train(model, device, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)               # 各个分类的概率\n",
    "        loss = criterion(out, y)\n",
    "        train_loss += loss\n",
    "        acc = accu(out, y)\n",
    "        train_acc += acc\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return train_loss / len(train_loader), train_acc / len(train_loader) \n",
    "\n",
    "def evaluate(model, device, test_loader, criterion, optimizer):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            out = model(x)               \n",
    "            loss = criterion(out, y)\n",
    "            test_loss += loss\n",
    "            acc = accu(out, y)\n",
    "            test_acc += acc\n",
    "    return test_loss/len(test_loader), test_acc/len(test_loader)\n",
    "\n",
    "def modify_lr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(\"pre modify lr is:\", param_group[\"lr\"])\n",
    "        param_group[\"lr\"] = lr\n",
    "        print(\"after modify lr is:\", param_group[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trans = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomCrop(32,padding=3),\n",
    "    transforms.Resize(227),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.49139968, 0.48215827, 0.44653124), (0.24703233, 0.24348505, 0.26158768))#参数mean和std来自于训练集，但是transform本身会在训练和评测的时候都会使用\n",
    "])\n",
    "\n",
    "data_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(227),\n",
    "    transforms.Normalize((0.49139968, 0.48215827, 0.44653124), (0.24703233, 0.24348505, 0.26158768))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_data = datasets.CIFAR10('data', train=True, download=True, transform=data_trans)\n",
    "test_data = datasets.CIFAR10('data', train=False, download=True, transform=data_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(len(train_data)*0.9)\n",
    "n_valid = len(train_data) - n_train\n",
    "train_datasets, valid_datasets = torch.utils.data.random_split(train_data, [n_train, n_valid])\n",
    "\n",
    "train_loader = DataLoader(train_datasets, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_datasets, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AlexNet(10)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "    model = model.to(device)\n",
    "else:\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(),lr=1e-1)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1|Train Loss:1.9985065460205078|Train Acc:0.24900053304904052|Val Loss:1.9470514059066772|Val Acc:0.2921974522292994\n",
      "Epoch:2|Train Loss:1.606934666633606|Train Acc:0.40984808102345416|Val Loss:1.6094648838043213|Val Acc:0.41182324840764334\n",
      "Epoch:3|Train Loss:1.3857431411743164|Train Acc:0.5034648187633263|Val Loss:1.7136858701705933|Val Acc:0.4408837579617834\n",
      "Epoch:4|Train Loss:1.2335546016693115|Train Acc:0.5627887348969438|Val Loss:1.187134861946106|Val Acc:0.5833996815286624\n",
      "Epoch:5|Train Loss:1.1408305168151855|Train Acc:0.6030339374555792|Val Loss:1.0878983736038208|Val Acc:0.621218152866242\n",
      "Epoch:6|Train Loss:1.0741417407989502|Train Acc:0.625022210376688|Val Loss:1.2782827615737915|Val Acc:0.5408041401273885\n",
      "Epoch:7|Train Loss:1.0318268537521362|Train Acc:0.6456334399431415|Val Loss:1.3915481567382812|Val Acc:0.5195063694267515\n",
      "Epoch:8|Train Loss:1.0064822435379028|Train Acc:0.6534737029140014|Val Loss:0.9992812871932983|Val Acc:0.6590366242038217\n",
      "Epoch:9|Train Loss:0.9851111173629761|Train Acc:0.6627576403695806|Val Loss:0.9560263156890869|Val Acc:0.6723726114649682\n",
      "pre modify lr is: 0.1\n",
      "after modify lr is: 0.01\n",
      "Epoch:10|Train Loss:0.9729580283164978|Train Acc:0.6706867448471926|Val Loss:0.9635166525840759|Val Acc:0.6648089171974523\n",
      "Epoch:11|Train Loss:0.7396594285964966|Train Acc:0.7470460199004975|Val Loss:0.6925520896911621|Val Acc:0.7641321656050956\n",
      "Epoch:12|Train Loss:0.6720348000526428|Train Acc:0.7709665955934613|Val Loss:0.6697162389755249|Val Acc:0.772093949044586\n",
      "Epoch:13|Train Loss:0.634713888168335|Train Acc:0.7823160980810234|Val Loss:0.6401488184928894|Val Acc:0.7810509554140127\n",
      "Epoch:14|Train Loss:0.6059107780456543|Train Acc:0.7915778251599147|Val Loss:0.6125091910362244|Val Acc:0.7894108280254777\n",
      "pre modify lr is: 0.01\n",
      "after modify lr is: 0.001\n",
      "Epoch:15|Train Loss:0.5876011848449707|Train Acc:0.7980410447761194|Val Loss:0.6082056760787964|Val Acc:0.7931926751592356\n",
      "Epoch:16|Train Loss:0.5620831251144409|Train Acc:0.8044376332622601|Val Loss:0.5957932472229004|Val Acc:0.8021496815286624\n",
      "Epoch:17|Train Loss:0.5541994571685791|Train Acc:0.8081911869225302|Val Loss:0.6004893183708191|Val Acc:0.7973726114649682\n",
      "Epoch:18|Train Loss:0.5464316010475159|Train Acc:0.8109674840085288|Val Loss:0.5916975736618042|Val Acc:0.8005573248407644\n",
      "Epoch:19|Train Loss:0.5496723055839539|Train Acc:0.8100790689410092|Val Loss:0.589020848274231|Val Acc:0.7997611464968153\n",
      "Epoch:20|Train Loss:0.5484268069267273|Train Acc:0.8107675906183369|Val Loss:0.5941344499588013|Val Acc:0.7939888535031847\n"
     ]
    }
   ],
   "source": [
    "model_path = './models/alexnet-sifar10.pth'\n",
    "best_valid_loss = float(\"inf\")\n",
    "info = 'Epoch:{0}|Train Loss:{1}|Train Acc:{2}|Val Loss:{3}|Val Acc:{4}'\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train(model, device, train_loader, criterion, optimizer)\n",
    "    valid_loss, valid_acc = evaluate(model, device, valid_loader, criterion, optimizer)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    if epoch+1 == 10:\n",
    "        modify_lr(optimizer, lr=1e-2)\n",
    "    if epoch+1 == 15:\n",
    "        modify_lr(optimizer, lr=1e-3)\n",
    "    print(info.format(epoch+1, train_loss, train_acc, valid_loss, valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图像增强\n",
    "### torchvision.transforms 包括所有图像增强的方法 。\n",
    "- Scale，对图片的尺度进行缩小和放大;\n",
    "- RandomRotation 对图片随机旋转；\n",
    "- CenterCrop，对图像正中心进行给定大小的裁剪;\n",
    "- RandomCrop，对图片进行给定大小的随机裁剪;\n",
    "- RandomHorizaontalFlip，对图片进行概率为0.5的随机水平翻转;\n",
    "- RandomSizedCrop，首先对图片进行随机尺寸的裁剪，然后对裁剪的图片进行一个随机比例的缩放，最后将图片变成给定的大小，这在Inception Net 中比较流行;\n",
    "- 最后一个是Pad，对图片进行边界零填充。\n",
    "\n",
    "上面介绍了 PyTorch 内置的一些图像增强的方法，还有更多的增强方法见[transforms的二十二个方法](https://zhuanlan.zhihu.com/p/53367135)，可以使用OpenCV或者PIL等第二方图形库实现。在网络的训练中图像增强是一种常见、默认的做法，对多任务进行图像增强之后都能够在一定程度上提升任务的准确率。\n",
    "\n",
    "- 本篇幅参考[经典CNN网络 - AlexNet总结](https://juejin.im/post/5ad173b651882555731c8f9b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet优点/AlexNet高性能的解释\n",
    "- 非线性激活函数：ReLU\n",
    "- 防止过拟合的方法：Dropout，Data augmentation\n",
    "- 大数据训练：百万级ImageNet图像数据\n",
    "- 其他：GPU实现，LRN归一化层的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD与Adam\n",
    "- 有时候Adam不收敛，效果很差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
